{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-17T21:39:59.602155Z",
     "start_time": "2025-07-17T21:39:59.598781Z"
    }
   },
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Cell 1: Imports & Vertex AI initialization\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.featurestore import Featurestore, EntityType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import component, Input, Output, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# --- YOUR SETTINGS ---\n",
    "PROJECT_ID = \"mlops-466217\"\n",
    "REGION     = \"us-central1\"\n",
    "BUCKET     = \"gs://mlops-assignment2-bucket\"\n",
    "FEATURESTORE_ID = \"athletes_feature_store\"\n",
    "ENTITY_TYPE     = \"athlete\"\n",
    "PIPELINE_ROOT    = f\"{BUCKET}/pipeline_root\"\n",
    "# ---------------------\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T21:40:00.269255Z",
     "start_time": "2025-07-17T21:39:59.623323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 2: Load CSV & basic preprocessing\n",
    "df = pd.read_csv(f\"{BUCKET}/athletes_cleaned.csv\")\n",
    "# normalize column names\n",
    "df.columns = (\n",
    "    df.columns\n",
    "      .str.strip()\n",
    "      .str.lower()\n",
    "      .str.replace(\" \", \"_\")\n",
    ")\n",
    "df[\"athlete_id\"] = df.index\n",
    "df.index = df[\"athlete_id\"]\n",
    "\n",
    "# compute target\n",
    "df[\"target\"] = df[[\"candj\",\"snatch\",\"deadlift\",\"backsq\"]].sum(axis=1)\n",
    "\n",
    "# split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
   ],
   "id": "14e5d1baf39bc7dc",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T21:40:01.835048Z",
     "start_time": "2025-07-17T21:40:01.443619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 2.6: Encode pipe‚Äëdelimited multi‚Äëselect columns\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "multi_select_cols = [\"background\", \"eat\", \"experience\", \"schedule\", \"howlong\"]  # add any others here\n",
    "\n",
    "for col in multi_select_cols:\n",
    "    # 1. Split each string into a list\n",
    "    train_lists = train_df[col].fillna(\"\").str.split(r\"\\s*\\|\\s*\")\n",
    "    test_lists  = test_df[col].fillna(\"\").str.split(r\"\\s*\\|\\s*\")\n",
    "\n",
    "    # 2. Fit a MultiLabelBinarizer on train, then transform both\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    train_enc = mlb.fit_transform(train_lists)\n",
    "    test_enc  = mlb.transform(test_lists)\n",
    "\n",
    "    # 3. Build generic column names: col_1, col_2, ‚Ä¶ up to n_options\n",
    "    #    (or you can sanitize mlb.classes_ into names if you prefer)\n",
    "    new_cols = [f\"{col}_{i+1}\" for i in range(len(mlb.classes_))]\n",
    "\n",
    "    # 4. Create DataFrames and concat\n",
    "    train_ohe = pd.DataFrame(train_enc, columns=new_cols, index=train_df.index)\n",
    "    test_ohe  = pd.DataFrame(test_enc,  columns=new_cols, index=test_df.index)\n",
    "\n",
    "    train_df = pd.concat([train_df.drop(columns=[col]), train_ohe], axis=1)\n",
    "    test_df  = pd.concat([test_df.drop(columns=[col]), test_ohe],  axis=1)\n",
    "\n",
    "print(\"‚úÖ Multi‚Äëselect columns encoded:\", multi_select_cols)\n",
    "print(train_df.columns)"
   ],
   "id": "191b6eb76ac8f924",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi‚Äëselect columns encoded: ['background', 'eat', 'experience', 'schedule', 'howlong']\n",
      "Index(['unnamed:_0', 'region', 'gender', 'age', 'height', 'weight', 'candj',\n",
      "       'snatch', 'deadlift', 'backsq', 'athlete_id', 'target', 'background_1',\n",
      "       'background_2', 'background_3', 'background_4', 'background_5',\n",
      "       'background_6', 'background_7', 'eat_1', 'eat_2', 'eat_3', 'eat_4',\n",
      "       'eat_5', 'eat_6', 'eat_7', 'experience_1', 'experience_2',\n",
      "       'experience_3', 'experience_4', 'experience_5', 'experience_6',\n",
      "       'experience_7', 'experience_8', 'schedule_1', 'schedule_2',\n",
      "       'schedule_3', 'schedule_4', 'schedule_5', 'schedule_6', 'schedule_7',\n",
      "       'schedule_8', 'schedule_9', 'howlong_1', 'howlong_2', 'howlong_3',\n",
      "       'howlong_4', 'howlong_5', 'howlong_6', 'howlong_7'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T21:40:07.246274Z",
     "start_time": "2025-07-17T21:40:07.234676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 3: Feature Engineering Versions\n",
    "def fe_v1(df):\n",
    "    df = df.copy()\n",
    "    df[\"bmi\"] = df[\"weight\"] / (df[\"height\"] ** 2)\n",
    "    df[\"pct_candj_bw\"] = df[\"candj\"] / df[\"weight\"]\n",
    "    return df\n",
    "\n",
    "def fe_v2(df):\n",
    "    df = fe_v1(df)\n",
    "    df[\"deadlift_snatch_ratio\"] = df[\"deadlift\"] / (df[\"snatch\"] + 1e-6)\n",
    "    return df\n",
    "\n",
    "train_v1 = fe_v1(train_df)\n",
    "test_v1  = fe_v1(test_df)\n",
    "train_v2 = fe_v2(train_df)\n",
    "test_v2  = fe_v2(test_df)\n"
   ],
   "id": "d0fcf28d2d31dc82",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T21:40:10.638675Z",
     "start_time": "2025-07-17T21:40:09.375208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 4: Create (or fetch) Feature Store + EntityType\n",
    "try:\n",
    "    fs = Featurestore.create(\n",
    "      featurestore_id=FEATURESTORE_ID,\n",
    "      online_store_fixed_node_count=1\n",
    "    )\n",
    "    print(\"üì¶ Featurestore created\")\n",
    "except Exception:\n",
    "    fs = Featurestore(FEATURESTORE_ID)\n",
    "    print(\"üì¶ Featurestore exists\")\n",
    "\n",
    "try:\n",
    "    et = fs.create_entity_type(entity_type_id=ENTITY_TYPE)\n",
    "    print(\"üîë EntityType created\")\n",
    "except Exception:\n",
    "    et = fs.get_entity_type(entity_type_id=ENTITY_TYPE)\n",
    "    print(\"üîë EntityType exists\")\n"
   ],
   "id": "ea0e6998c4699c42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Featurestore exists\n",
      "üîë EntityType exists\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 5: Upload CSVs via Python API & ingest into Feature Store\n",
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "\n",
    "# initialize GCS client & bucket\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "bucket_name = BUCKET.replace(\"gs://\", \"\")\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "# helper to upload a local file to GCS\n",
    "def upload_to_gcs(local_path: str, gcs_path: str):\n",
    "    blob = bucket.blob(gcs_path)\n",
    "    blob.upload_from_filename(local_path)\n",
    "    print(f\"‚úîÔ∏è Uploaded {local_path} ‚Üí gs://{bucket_name}/{gcs_path}\")\n",
    "\n",
    "# -- Version 1 features --\n",
    "# 1) write CSV locally\n",
    "train_v1.to_csv(\"/tmp/train_v1.csv\", index=False)\n",
    "\n",
    "# 2) upload to GCS\n",
    "upload_to_gcs(\"/tmp/train_v1.csv\", \"features/train_v1.csv\")\n",
    "\n",
    "# 3) ingest CSV\n",
    "et.ingest_from_gcs(\n",
    "    feature_ids=[\"bmi\", \"pct_candj_bw\"],\n",
    "    gcs_source_uris=[f\"{BUCKET}/features/train_v1.csv\"],\n",
    "    gcs_source_type=\"csv\",\n",
    "    entity_id_field=\"athlete_id\",\n",
    "    feature_time=datetime.now(),\n",
    "    worker_count=4,\n",
    ")"
   ],
   "id": "fffe9a87ae0a023",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T21:53:40.566815Z",
     "start_time": "2025-07-17T21:53:38.823298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 5.1: Ensure v2 feature schema exists\n",
    "from google.api_core.exceptions import AlreadyExists\n",
    "\n",
    "try:\n",
    "    et.create_feature(\n",
    "        feature_id=\"deadlift_snatch_ratio\",\n",
    "        value_type=\"DOUBLE\"\n",
    "    )\n",
    "    print(\"‚úÖ Created feature `deadlift_snatch_ratio`\")\n",
    "except AlreadyExists:\n",
    "    print(\"‚ÑπÔ∏è Feature `deadlift_snatch_ratio` already exists\")\n"
   ],
   "id": "6e250746d9330a04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Feature\n",
      "Create Feature backing LRO: projects/472721448719/locations/us-central1/featurestores/athletes_feature_store/entityTypes/athlete/features/deadlift_snatch_ratio/operations/677273564704931840\n",
      "Feature created. Resource name: projects/472721448719/locations/us-central1/featurestores/athletes_feature_store/entityTypes/athlete/features/deadlift_snatch_ratio\n",
      "To use this Feature in another session:\n",
      "feature = aiplatform.Feature('projects/472721448719/locations/us-central1/featurestores/athletes_feature_store/entityTypes/athlete/features/deadlift_snatch_ratio')\n",
      "‚úÖ Created feature `deadlift_snatch_ratio`\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T21:59:27.094996Z",
     "start_time": "2025-07-17T21:53:43.184175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -- Version 2 features (adds deadlift_snatch_ratio) --\n",
    "train_v2.to_csv(\"/tmp/train_v2.csv\", index=False)\n",
    "upload_to_gcs(\"/tmp/train_v2.csv\", \"features/train_v2.csv\")\n",
    "\n",
    "et.ingest_from_gcs(\n",
    "    feature_ids=[\"bmi\", \"pct_candj_bw\", \"deadlift_snatch_ratio\"],\n",
    "    gcs_source_uris=[f\"{BUCKET}/features/train_v2.csv\"],\n",
    "    gcs_source_type=\"csv\",\n",
    "    entity_id_field=\"athlete_id\",\n",
    "    feature_time=datetime.now(),\n",
    "    worker_count=4,\n",
    ")\n"
   ],
   "id": "a7d2a7cdec6a3e7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Uploaded /tmp/train_v2.csv ‚Üí gs://mlops-assignment2-bucket/features/train_v2.csv\n",
      "Importing EntityType feature values: projects/472721448719/locations/us-central1/featurestores/athletes_feature_store/entityTypes/athlete\n",
      "Import EntityType feature values backing LRO: projects/472721448719/locations/us-central1/featurestores/athletes_feature_store/entityTypes/athlete/operations/2120677245277175808\n",
      "EntityType feature values imported. Resource name: projects/472721448719/locations/us-central1/featurestores/athletes_feature_store/entityTypes/athlete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.featurestore.entity_type.EntityType object at 0x7f084d0fa360> \n",
       "resource name: projects/472721448719/locations/us-central1/featurestores/athletes_feature_store/entityTypes/athlete"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T22:46:57.367855Z",
     "start_time": "2025-07-17T22:46:57.362393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 6: Training component (batched et.read to avoid 100‚ÄëID limit)\n",
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"scikit-learn\",\n",
    "        \"pandas\",\n",
    "        \"numpy\",\n",
    "        \"codecarbon\"\n",
    "    ]\n",
    ")\n",
    "def train_on_fs(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    featurestore_id: str,\n",
    "    entity_type_id: str,\n",
    "    feature_ids: list,\n",
    "    source_csv_uri: str,\n",
    "    hyperparameters: dict,\n",
    "    metrics_output: Output[Dataset],\n",
    "):\n",
    "    import os, json, math\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    from codecarbon import EmissionsTracker\n",
    "    from google.cloud import aiplatform, storage\n",
    "\n",
    "    # 1) Init clients\n",
    "    aiplatform.init(project=project, location=region)\n",
    "    fs = aiplatform.Featurestore(featurestore_id)\n",
    "    et = fs.get_entity_type(entity_type_id)\n",
    "    storage_client = storage.Client(project=project)\n",
    "\n",
    "    # 2) Download the ingested CSV to get all athlete IDs\n",
    "    bucket_name, path = source_csv_uri.replace(\"gs://\", \"\").split(\"/\", 1)\n",
    "    blob = storage_client.bucket(bucket_name).blob(path)\n",
    "    local_csv = \"/tmp/source.csv\"\n",
    "    blob.download_to_filename(local_csv)\n",
    "    raw_df = pd.read_csv(local_csv)\n",
    "    entity_ids = raw_df[\"athlete_id\"].astype(str).tolist()\n",
    "\n",
    "    # 3) Batch‚Äëread latest feature values in chunks of 100 IDs\n",
    "    cols = feature_ids + [\"candj\", \"snatch\", \"deadlift\", \"backsq\"]\n",
    "    dfs = []\n",
    "    batch_size = 100\n",
    "    for i in range(0, len(entity_ids), batch_size):\n",
    "        batch = entity_ids[i : i + batch_size]\n",
    "        batch_df = (\n",
    "            et.read(entity_ids=batch, feature_ids=cols)\n",
    "              .to_dataframe()\n",
    "        )\n",
    "        dfs.append(batch_df)\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # 4) Prepare X/y\n",
    "    df[\"target\"] = df[[\"candj\",\"snatch\",\"deadlift\",\"backsq\"]].sum(axis=1)\n",
    "    X = df[feature_ids]\n",
    "    y = df[\"target\"]\n",
    "\n",
    "    # 5) Train & track carbon\n",
    "    tracker = EmissionsTracker()\n",
    "    tracker.start()\n",
    "    model = RandomForestRegressor(**hyperparameters)\n",
    "    model.fit(X, y)\n",
    "    emissions = tracker.stop()\n",
    "\n",
    "    # 6) Evaluate\n",
    "    preds = model.predict(X)\n",
    "    mse = mean_squared_error(y, preds)\n",
    "    r2 = r2_score(y, preds)\n",
    "\n",
    "    # 7) Write metrics.json\n",
    "    os.makedirs(metrics_output.path, exist_ok=True)\n",
    "    with open(os.path.join(metrics_output.path, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump({\"mse\": mse, \"r2\": r2, \"co2_kg\": emissions}, f)\n"
   ],
   "id": "c485768e0464ddbd",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T22:46:59.330710Z",
     "start_time": "2025-07-17T22:46:59.246323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 7: Define the Vertex AI Pipeline\n",
    "@dsl.pipeline(\n",
    "    name=\"athlete-exp-pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def athlete_pipeline(\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION,\n",
    "    fs_id: str = FEATURESTORE_ID,\n",
    "    et_id: str = ENTITY_TYPE,\n",
    "    bucket: str = BUCKET,\n",
    "):\n",
    "    def add_train_step(version_name: str, feats: list, params: dict, csv_path: str):\n",
    "        step = train_on_fs(\n",
    "            project=project,\n",
    "            region=region,\n",
    "            featurestore_id=fs_id,\n",
    "            entity_type_id=et_id,\n",
    "            feature_ids=feats,\n",
    "            source_csv_uri=f\"{bucket}/{csv_path}\",\n",
    "            hyperparameters=params,\n",
    "        ).set_cpu_limit(\"32\") \\\n",
    "         .set_memory_limit(\"64Gi\")\n",
    "        return step\n",
    "\n",
    "    # version‚Äë1 CSV is at features/train_v1.csv\n",
    "    t1 = add_train_step(\n",
    "        \"train-v1-A\",\n",
    "        [\"bmi\", \"pct_candj_bw\"],\n",
    "        {\"n_estimators\": 100, \"random_state\": 42},\n",
    "        \"features/train_v1.csv\"\n",
    "    )\n",
    "    t2 = add_train_step(\n",
    "        \"train-v1-B\",\n",
    "        [\"bmi\", \"pct_candj_bw\"],\n",
    "        {\"n_estimators\": 200, \"random_state\": 42},\n",
    "        \"features/train_v1.csv\"\n",
    "    )\n",
    "\n",
    "    # version‚Äë2 CSV is at features/train_v2.csv\n",
    "    t3 = add_train_step(\n",
    "        \"train-v2-A\",\n",
    "        [\"bmi\", \"pct_candj_bw\", \"deadlift_snatch_ratio\"],\n",
    "        {\"n_estimators\": 100, \"random_state\": 42},\n",
    "        \"features/train_v2.csv\"\n",
    "    )\n",
    "    t4 = add_train_step(\n",
    "        \"train-v2-B\",\n",
    "        [\"bmi\", \"pct_candj_bw\", \"deadlift_snatch_ratio\"],\n",
    "        {\"n_estimators\": 200, \"random_state\": 42},\n",
    "        \"features/train_v2.csv\"\n",
    "    )\n"
   ],
   "id": "cc01c662e95b341c",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-17T22:47:01.508893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 8: Compile & Submit the Pipeline\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=athlete_pipeline,\n",
    "    package_path=\"athlete_pipeline.json\"\n",
    ")\n",
    "\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"athlete_inside_fs_experiments\",\n",
    "    template_path=\"athlete_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "pipeline_job.run(sync=True)\n"
   ],
   "id": "a2c348d90fdde8ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/472721448719/locations/us-central1/pipelineJobs/athlete-exp-pipeline-20250717174701\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/472721448719/locations/us-central1/pipelineJobs/athlete-exp-pipeline-20250717174701')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/athlete-exp-pipeline-20250717174701?project=472721448719\n",
      "PipelineJob projects/472721448719/locations/us-central1/pipelineJobs/athlete-exp-pipeline-20250717174701 current state:\n",
      "3\n",
      "PipelineJob projects/472721448719/locations/us-central1/pipelineJobs/athlete-exp-pipeline-20250717174701 current state:\n",
      "3\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 9: Fetch metrics & compare locally\n",
    "import json\n",
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client(project=PROJECT_ID)\n",
    "bucket = client.bucket(BUCKET.replace(\"gs://\",\"\"))\n",
    "results = []\n",
    "names = [\"t1\",\"t2\",\"t3\",\"t4\"]\n",
    "for name in names:\n",
    "    # adjust your prefix to where the pipeline wrote artifacts\n",
    "    prefix = f\"pipeline_root/athlete_inside_fs_experiments/{pipeline_job.job_id}/{name}/metrics.json\"\n",
    "    blob = bucket.blob(prefix)\n",
    "    metrics = json.loads(blob.download_as_string())\n",
    "    results.append(metrics)\n",
    "\n",
    "df_res = pd.DataFrame(results, index=[\"v1_100\",\"v1_200\",\"v2_100\",\"v2_200\"])\n",
    "print(df_res)\n",
    "\n",
    "# quantitative: MSE & R¬≤\n",
    "df_res[[\"mse\",\"r2\"]].plot.bar(subplots=True, layout=(1,2), figsize=(8,4))\n",
    "plt.suptitle(\"Model Metrics Comparison\")\n",
    "\n",
    "# qualitative: emissions\n",
    "df_res[\"co2_kg\"].plot.bar(figsize=(6,4))\n",
    "plt.title(\"Carbon Emissions (kg CO‚ÇÇ)\")\n",
    "plt.ylabel(\"kg CO‚ÇÇ\")\n",
    "plt.show()\n"
   ],
   "id": "1563583e8a65233f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a28bbc75663b7e5b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
